{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 1: Extração de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.9'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Exibir versão do Python\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.5.1-py2.py3-none-any.whl (254 kB)\n",
      "\u001b[K     |████████████████████████████████| 254 kB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h2<4.0,>=3.0\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 1.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyOpenSSL>=16.2.0\n",
      "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cryptography>=2.0\n",
      "  Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 19.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Downloading zope.interface-5.4.0-cp39-cp39-manylinux2010_x86_64.whl (255 kB)\n",
      "\u001b[K     |████████████████████████████████| 255 kB 66.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lxml>=3.5.0\n",
      "  Downloading lxml-4.6.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.9 MB 23.9 MB/s eta 0:00:01     |██████████████████████████▌     | 5.7 MB 23.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.4.0-py3-none-any.whl (10 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Twisted[http2]>=17.9.0\n",
      "  Downloading Twisted-21.7.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 31.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in ./venv/lib/python3.9/site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: six>=1.6.0 in ./venv/lib/python3.9/site-packages (from parsel>=1.5.0->scrapy) (1.16.0)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 3.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 32.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in ./venv/lib/python3.9/site-packages (from service-identity>=16.0.0->scrapy) (21.2.0)\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting incremental>=21.3.0\n",
      "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting typing-extensions>=3.6.5\n",
      "  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting priority<2.0,>=1.1.0\n",
      "  Downloading priority-1.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting idna>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from zope.interface>=4.1.3->scrapy) (58.1.0)\n",
      "Using legacy 'setup.py install' for protego, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for PyDispatcher, since package 'wheel' is not installed.\n",
      "Installing collected packages: idna, zope.interface, w3lib, typing-extensions, pyasn1, lxml, incremental, hyperlink, hyperframe, hpack, cssselect, constantly, Automat, Twisted, pyasn1-modules, priority, parsel, jmespath, itemadapter, h2, cryptography, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n",
      "    Running setup.py install for PyDispatcher ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for protego ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-21.7.0 constantly-15.1.0 cryptography-36.0.0 cssselect-1.1.0 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 hyperlink-21.0.0 idna-3.3 incremental-21.3.0 itemadapter-0.4.0 itemloaders-1.0.4 jmespath-0.10.0 lxml-4.6.4 parsel-1.6.0 priority-1.3.0 protego-0.1.16 pyOpenSSL-21.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.6.2 scrapy-2.5.1 service-identity-21.1.0 typing-extensions-4.0.1 w3lib-1.22.0 zope.interface-5.4.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/gabrielfv/Projects/UFC/ck0223-mineracao/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try: # Checando se Scrapy está instalado\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    # URLs\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/'\n",
    "    ]\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "       \n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract(),\n",
    "                'author': quote.css('span small::text').extract(),\n",
    "                'tags': quote.css('div.tags a.tag::text').extract()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 16:01:03 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2021-12-11 16:01:03 [scrapy.utils.log] INFO: Versions: lxml 4.6.4.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.9 (main, Nov 20 2021, 21:30:06) - [GCC 11.1.0], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 36.0.0, Platform Linux-5.15.7-arch1-1-x86_64-with-glibc2.33\n",
      "2021-12-11 16:01:03 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2021-12-11 16:01:03 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2021-12-11 16:01:03 [scrapy.extensions.telnet] INFO: Telnet Password: 5bdb719deae0a6e6\n",
      "2021-12-11 16:01:03 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-12-11 16:01:03 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-12-11 16:01:03 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-12-11 16:01:03 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-12-11 16:01:03 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-12-11 16:01:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-12-11 16:01:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7f4e64e9ff70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 16:01:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n",
      "2021-12-11 16:01:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
      "{'text': ['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'], 'author': ['Albert Einstein'], 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
      "2021-12-11 16:01:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
      "{'text': ['“It is our choices, Harry, that show what we truly are, far more than our abilities.”'], 'author': ['J.K. Rowling'], 'tags': ['abilities', 'choices']}\n",
      "2021-12-11 16:01:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
      "{'text': ['“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”'], 'author': ['Albert Einstein'], 'tags': ['inspirational', 'life', 'live', 'miracle', 'miracles']}\n",
      "2021-12-11 16:01:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
      "{'text': ['“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”'], 'author': ['Jane Austen'], 'tags': ['aliteracy', 'books', 'classic', 'humor']}\n",
      "2021-12-11 16:01:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
      "{'text': [\"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\"], 'author': ['Marilyn Monroe'], 'tags': ['be-yourself', 'inspirational']}\n",
      "2021-12-11 16:01:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
      "{'text': ['“Try not to become a man of success. Rather become a man of value.”'], 'author': ['Albert Einstein'], 'tags': ['adulthood', 'success', 'value']}\n",
      "2021-12-11 16:01:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
      "{'text': ['“It is better to be hated for what you are than to be loved for what you are not.”'], 'author': ['André Gide'], 'tags': ['life', 'love']}\n",
      "2021-12-11 16:01:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
      "{'text': [\"“I have not failed. I've just found 10,000 ways that won't work.”\"], 'author': ['Thomas A. Edison'], 'tags': ['edison', 'failure', 'inspirational', 'paraphrased']}\n",
      "2021-12-11 16:01:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
      "{'text': [\"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\"], 'author': ['Eleanor Roosevelt'], 'tags': ['misattributed-eleanor-roosevelt']}\n",
      "2021-12-11 16:01:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
      "{'text': ['“A day without sunshine is like, you know, night.”'], 'author': ['Steve Martin'], 'tags': ['humor', 'obvious', 'simile']}\n",
      "2021-12-11 16:01:04 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-12-11 16:01:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 226,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2204,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.73709,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 12, 11, 19, 1, 4, 211218),\n",
      " 'httpcompression/response_bytes': 11053,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/DEBUG': 11,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 91340800,\n",
      " 'memusage/startup': 91340800,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2021, 12, 11, 19, 1, 3, 474128)}\n",
      "2021-12-11 16:01:04 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "# Iniciando processo\n",
    "process.crawl(QuotesSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 2: Gerando Arquivo de Saída"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ao executar esse trecho, reinicie o jupyter notebook"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "OBS: É importante que o kernel desse Jupyter Notebook seja restartado e reexecutado a partir daqui!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Exibir versão do Python\n",
    "import platform\n",
    "platform.python_version()\n",
    "\n",
    "try: # Checando se Scrapy está instalado\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('quoteresult.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    # URLs\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "    \n",
    "    # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'quoteresult.json'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract()[0],\n",
    "                'author': quote.css('span small::text').extract()[0],\n",
    "                'tags': quote.css('div.tags a.tag::text').extract()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 16:01:46 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2021-12-11 16:01:46 [scrapy.utils.log] INFO: Versions: lxml 4.6.4.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.9 (main, Nov 20 2021, 21:30:06) - [GCC 11.1.0], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 36.0.0, Platform Linux-5.15.7-arch1-1-x86_64-with-glibc2.33\n",
      "2021-12-11 16:01:46 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2021-12-11 16:01:46 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n",
      "2021-12-11 16:01:46 [py.warnings] WARNING: /home/gabrielfv/Projects/UFC/ck0223-mineracao/venv/lib/python3.9/site-packages/scrapy/extensions/feedexport.py:247: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7feaaf2dd040>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "# Iniciando processo\n",
    "process.crawl(QuotesSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Carregando JSON criado para visualizar saída\n",
    "output = pd.read_json('quoteresult.jl', lines=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outro Exemplo: site SEFAZ"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://normas.receita.fazenda.gov.br/sijut2consulta/consulta.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Checando se Scrapy está instalado\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('normaresult.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SefazItem(scrapy.Item):\n",
    "    decreto = scrapy.Field()\n",
    "    publicacao_dou = scrapy.Field()\n",
    "    ementa = scrapy.Field()\n",
    "    norma = scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publicacao_dou(response):\n",
    "    publicacao_dou = response.css('div.tituloPublicacao ::text')[0].extract().strip()\n",
    "        \n",
    "    return publicacao_dou\n",
    "\n",
    "\n",
    "def get_ementa(response):\n",
    "    # retrieve the ementa <div>\n",
    "    ementa_div = response.css('p.ementa::text').extract()\n",
    "\n",
    "    ementa_list = []\n",
    "    for i in ementa_div:\n",
    "        ementa_list.append(i.strip())\n",
    "\n",
    "    # that ementa_list is broken into some sentences; the function below generates the joint text\n",
    "    ementa = ' '.join(ementa_list)\n",
    "    return ementa\n",
    "\n",
    "def get_norma_elements(response):\n",
    "    # retrieve the norma <div>\n",
    "    norma = response.css('div.divSegmentos')\n",
    "\n",
    "    # declaring the list\n",
    "    norma_list = []\n",
    "    for i in norma:\n",
    "        # retrieve only the text <p> inside the <div>\n",
    "        tmp = i.css('span::text')[0].extract().strip()\n",
    "        # add each element into the norma list\n",
    "        norma_list.append(tmp)\n",
    "\n",
    "    return norma_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class NormaSpider(scrapy.Spider):\n",
    "    name = 'norma'\n",
    "    start_urls = [\n",
    "        'http://normas.receita.fazenda.gov.br/sijut2consulta/link.action?visao=anotado&idAto=95072'\n",
    "    ]\n",
    "\n",
    "    # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'normaresult.json'                        \n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items = SefazItem()\n",
    "        items['publicacao_dou'] = get_publicacao_dou(response)\n",
    "        items['ementa'] = get_ementa(response)\n",
    "        items['norma'] = get_norma_elements(response)\n",
    "\n",
    "        yield items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "# Iniciando processo\n",
    "process.crawl(NormaSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Carregando JSON criado para visualizar saída\n",
    "output = pd.read_json('normaresult.jl', lines=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pd.options.display.max_colwidth = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links Uteis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://docs.scrapy.org/en/latest/topics/item-pipeline.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=ve_0h4Y8nuI&list=PLhTjy8cBISEqkN-5Ku_kXG4QW33sxQo0t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
